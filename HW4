// Name - Robert Barrett
#include <sys/time.h>
#include <stdio.h>
#include <stdlib.h>
#include <math.h>


// Defines
#define N 1150300 // Length of the vector

// Global variables
float *A_CPU, *B_CPU, *C_CPU; // CPU pointers
float *A_GPU, *B_GPU, *C_GPU; // GPU pointers
dim3 BlockSize; // This variable will hold the Dimensions of your blocks
dim3 GridSize;  // This variable will hold the Dimensions of your grid
float Tolerance = 0.01;

// Function prototypes
void cudaErrorCheck(const char *, int);
void setUpDevices();
void allocateMemory();
void innitialize();
void addVectorsCPU(float*, float*, float*, int);
__global__ void addVectorsGPU(float*, float*, float*, int);
bool check(float*, int, float);
long elaspedTime(struct timeval, struct timeval);
void CleanUp();

// This check to see if an error happened in your CUDA code. It tells you what it thinks went wrong,
// and what file and line it occured on.
void cudaErrorCheck(const char *file, int line)
{
    cudaError_t  error;
    error = cudaGetLastError();

    if(error != cudaSuccess)
    {
        printf("\n CUDA ERROR: message = %s, File = %s, Line = %d\n", cudaGetErrorString(error), file, line);
        exit(0);
    }
}

// This will be the layout of the parallel space we will be using.
void setUpDevices()
{
    BlockSize.x = 1024;
    BlockSize.y = 1;
    BlockSize.z = 1;

    GridSize.x = 256; 
    GridSize.y = 1;
    GridSize.z = 1;
}

// Allocating the memory we will be using.
void allocateMemory()
{
    // Host "CPU" memory.
    A_CPU = (float*)malloc(N * sizeof(float));
    B_CPU = (float*)malloc(N * sizeof(float));
    C_CPU = (float*)malloc(N * sizeof(float));

    // Device "GPU" Memory
    cudaMalloc(&A_GPU, N * sizeof(float));
    cudaErrorCheck(__FILE__, __LINE__);
    cudaMalloc(&B_GPU, N * sizeof(float));
    cudaErrorCheck(__FILE__, __LINE__);
    cudaMalloc(&C_GPU, N * sizeof(float));
    cudaErrorCheck(__FILE__, __LINE__);
}

// Loading values into the vectors that we will add.
void innitialize()
{
    for(int i = 0; i < N; i++)
    {
        A_CPU[i] = (float)i;
        B_CPU[i] = (float)(2*i);
    }
}

// Adding vectors a and b on the CPU then stores result in vector c.
void addVectorsCPU(float *a, float *b, float *c, int n)
{
    for(int id = 0; id < n; id++)
    {
        // compute exactly the same formula used on GPU for parity
        float aval = a[id];
        float bval = b[id];

        float partA = sqrtf(cosf(aval)*cosf(aval) + aval*aval + sinf(aval)*sinf(aval) - 1.0f);
        float partB = sqrtf(cosf(bval)*cosf(bval) + bval*bval + sinf(bval)*sinf(bval) - 1.0f);
        c[id] = partA + partB;
    }
}

// GPU kernel: processes the vector using a for-stride loop so any grid/block configuration covers all elements.
// Also uses a small inner fixed-size loop which we hint to unroll.
__global__ void addVectorsGPU(float *a, float *b, float *c, int n)
{
    // starting index for this thread
    int start = blockIdx.x * blockDim.x + threadIdx.x;
    int stride = blockDim.x * gridDim.x;

    // process elements in a for-loop (for instead of while)
    for (int base = start; base < n; base += stride)
    {
        // we will attempt to process up to 4 consecutive elements per outer iteration to give the compiler
        // a chance to unroll the inner loop. The #pragma unroll helps the compiler unroll this small loop.
        #pragma unroll 4
        for (int off = 0; off < 4; ++off)
        {
            int id = base + off;
            if (id < n) // ensure we don't touch out-of-range memory
            {
                float aval = a[id];
                float bval = b[id];

                float partA = sqrtf(cosf(aval)*cosf(aval) + aval*aval + sinf(aval)*sinf(aval) - 1.0f);
                float partB = sqrtf(cosf(bval)*cosf(bval) + bval*bval + sinf(bval)*sinf(bval) - 1.0f);
                c[id] = partA + partB;
            }
        }
    }
}

// Checking to see if anything went wrong in the vector addition.
bool check(float *c, int n, float tolerance)
{
    int id;
    double myAnswer = 0.0;
    double trueAnswer;
    double percentError;
    double m = n - 1; // Needed the -1 because we start at 0.

    for(id = 0; id < n; id++)
    {
        myAnswer += c[id];
    }

    // trueAnswer for the original a(i)=i, b(i)=2i, and new c formula is no longer a simple closed-form like before.
    // For testing purposes recompute the CPU result and compare
    // We'll compute the "trueAnswer" by recomputing CPU-style (same as addVectorsCPU)
    double cpuSum = 0.0;
    for (id = 0; id < n; ++id)
    {
        float aval = (float)id;
        float bval = (float)(2*id);
        double partA = sqrt((double)cosf(aval)*cosf(aval) + (double)aval*aval + (double)sinf(aval)*sinf(aval) - 1.0);
        double partB = sqrt((double)cosf(bval)*cosf(bval) + (double)bval*bval + (double)sinf(bval)*sinf(bval) - 1.0);
        cpuSum += (partA + partB);
    }
    trueAnswer = cpuSum;

    percentError = fabs((myAnswer - trueAnswer)/trueAnswer) * 100.0;

    if(percentError < tolerance)
    {
        return(true);
    }
    else
    {
        printf("\n percentError = %f (tolerance %f)\n", percentError, tolerance);
        return(false);
    }
}

// Calculating elapsed time.
long elaspedTime(struct timeval start, struct timeval end)
{
    long startTime = start.tv_sec * 1000000 + start.tv_usec; // In microseconds.
    long endTime = end.tv_sec * 1000000 + end.tv_usec;       // In microseconds

    // Returning the total time elapsed in microseconds
    return endTime - startTime;
}

// Cleaning up memory after we are finished.
void CleanUp()
{
    // Freeing host "CPU" memory.
    free(A_CPU);
    free(B_CPU);
    free(C_CPU);

    cudaFree(A_GPU);
    cudaErrorCheck(__FILE__, __LINE__);
    cudaFree(B_GPU);
    cudaErrorCheck(__FILE__, __LINE__);
    cudaFree(C_GPU);
    cudaErrorCheck(__FILE__, __LINE__);
}

int main()
{
    timeval start, end;
    long timeCPU, timeGPU;

    // Setting up the GPU
    setUpDevices();

    // Allocating the memory you will need.
    allocateMemory();

    // Putting values in the vectors.
    innitialize();

    // Adding on the CPU
    gettimeofday(&start, NULL);
    addVectorsCPU(A_CPU, B_CPU ,C_CPU, N);
    gettimeofday(&end, NULL);
    timeCPU = elaspedTime(start, end);

    // Zeroing out the C_CPU vector just to be safe because right now it has the correct answer in it.
    for(int id = 0; id < N; id++)
    {
        C_CPU[id] = 0.0f;
    }

    // Adding on the GPU
    gettimeofday(&start, NULL);

    // Copy Memory from CPU to GPU
    // I keep your async copies, but if you remove async you can treat them as synchronous.
    cudaMemcpyAsync(A_GPU, A_CPU, N * sizeof(float), cudaMemcpyHostToDevice);
    cudaErrorCheck(__FILE__, __LINE__);
    cudaMemcpyAsync(B_GPU, B_CPU, N * sizeof(float), cudaMemcpyHostToDevice);
    cudaErrorCheck(__FILE__, __LINE__);

    // launch kernel
    addVectorsGPU<<<GridSize, BlockSize>>>(A_GPU, B_GPU, C_GPU, N);
    cudaErrorCheck(__FILE__, __LINE__);

    // Copy Memory from GPU to CPU
    cudaMemcpyAsync(C_CPU, C_GPU, N * sizeof(float), cudaMemcpyDeviceToHost);
    cudaErrorCheck(__FILE__, __LINE__);

    // synchronize
    cudaDeviceSynchronize();
    cudaErrorCheck(__FILE__, __LINE__);

    gettimeofday(&end, NULL);
    timeGPU = elaspedTime(start, end);

    // Checking to see if all went correctly.
    if(check(C_CPU, N, Tolerance) == false)
    {
        printf("\n\n Something went wrong in the GPU vector addition\n");
    }
    else
    {
        printf("\n\n You added the two vectors correctly on the GPU");
        printf("\n The time it took on the CPU was %ld microseconds", timeCPU);
        printf("\n The time it took on the GPU was %ld microseconds", timeGPU);
    }

    // You're done so cleanup your room.
    CleanUp();

    // Making sure it flushes out anything in the print buffer.
    printf("\n\n");

    return(0);
}
